# -*- coding: utf-8 -*-
"""LSTM model_Kerala.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17pO_6irarL0EaEMUYe4D1nUix0dP-4q0

# (1) Load the dataset and import libraries
"""

# Importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load Firepoints Data

kerala_clean_merge_data = pd.read_csv("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/KeralaFireWeatherAWS_merged.csv", encoding='latin1')
print(kerala_clean_merge_data.head())

"""## 1.1 Load the shape file of kerala"""

import geopandas as gpd
import matplotlib.pyplot as plt

# Load the shapefile
protect_area = gpd.read_file("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/StudyArea/Kerala_StateBoundary.shp")

# Preview spatial data
print(protect_area.head())

# Plot with axis labels and title
fig, ax = plt.subplots(figsize=(10, 8))
protect_area.plot(ax=ax, edgecolor='black')

# Add title and axis labels
ax.set_title("Kerala State Boundary (Western Ghat of India)", fontsize=14)
ax.set_xlabel("Longitude", fontsize=12)
ax.set_ylabel("Latitude", fontsize=12)

plt.grid(True)  # Optional: adds gridlines for better readability
plt.show()

"""# (2) Model implementation

## 2.1 Splitting data
"""

import numpy as np
import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler

# 0) Copy and ensure chronological order by timestamp
df = kerala_clean_merge_data.copy()
df['WX_TS'] = pd.to_datetime(df['WX_TS'], errors='coerce')  # ensure datetime type [24]
df = df.dropna(subset=['WX_TS']).sort_values('WX_TS').reset_index(drop=True)  # sort chronologically [21]

# 1) Select features (X) and targets (y)
feature_cols = [
    "TEMP. ('C)",
    "RH (%)",
    "RAIN FALL CUM. SINCE 0300 UTC (mm)",
    'WIND SPEED 10 m (Kt)'
]  # inputs [25]

target_cols = ['fire_occurrence','fire_count']  # multi-output target [25]

X = df[feature_cols].copy()
y = df[target_cols].copy()
t = df['WX_TS'].copy()

# 2) Chronological 70/15/15 split indices (no shuffle)
N = len(df)
train_end = int(N * 0.70)                 # 70% train [21]
val_end   = int(N * 0.85)                 # next 15% validation [21]

X_train, X_val, X_test = X.iloc[:train_end], X.iloc[train_end:val_end], X.iloc[val_end:]
y_train, y_val, y_test = y.iloc[:train_end], y.iloc[train_end:val_end], y.iloc[val_end:]
t_train, t_val, t_test = t.iloc[:train_end], t.iloc[train_end:val_end], t.iloc[val_end:]

# 3) Impute and scale using TRAIN-ONLY statistics to avoid leakage
imputer = SimpleImputer(strategy='median')                          # robust imputation [22]
scaler  = StandardScaler()                                          # z-score normalization [23]

X_train_imp = imputer.fit_transform(X_train)                        # fit on train only [22]
X_val_imp   = imputer.transform(X_val)                              # transform val [22]
X_test_imp  = imputer.transform(X_test)                             # transform test [22]

X_train_scaled = scaler.fit_transform(X_train_imp)                  # fit scaler on train only [22][23]
X_val_scaled   = scaler.transform(X_val_imp)                        # transform val [22]
X_test_scaled  = scaler.transform(X_test_imp)                       # transform test [22]

# 4) Build sliding-window sequences (samples, timesteps, features) for LSTM
def make_sequences(X_arr, y_df, t_series, lookback=48):
    """
    Create sliding-window sequences for LSTM:
      X_seq: (M, lookback, F)
      y_seq: (M, Y)  -> targets at the window end
      t_end: timestamps at window end
      end_idx: integer indices of window ends
    """
    # Validate aligned lengths
    N, F = X_arr.shape
    Y = y_df.shape[1]

    if N <= lookback:
        return (np.empty((0, lookback, F), dtype=X_arr.dtype),
                np.empty((0, Y), dtype=float),
                pd.Series([], dtype='datetime64[ns]'),
                np.array([], dtype=int))

    X_seq, y_seq, t_end, end_idx = [], [], [], []

    for i in range(lookback, N):
        X_seq.append(X_arr[i - lookback:i, :])
        y_seq.append(y_df.iloc[i].to_numpy())
        t_end.append(t_series.iloc[i])
        end_idx.append(i)

    return (np.asarray(X_seq),
            np.asarray(y_seq),
            pd.Series(t_end),
            np.asarray(end_idx, dtype=int))

# Choose lookback (e.g., 48 hourly steps) as temporal context for LSTM
lookback = 48  # adjust per your sampling frequency and accuracy needs [27][25]

# 5) Build sequences across the full series scaled by train stats, then mask into splits by end-index
X_all_scaled = np.vstack([X_train_scaled, X_val_scaled, X_test_scaled])  # scaled consistently via train stats [22]
y_all = pd.concat([y_train, y_val, y_test], axis=0).reset_index(drop=True)  # align targets [21]
t_all = pd.concat([t_train, t_val, t_test], axis=0).reset_index(drop=True)  # align times [21]

X_seq_all, y_seq_all, t_seq_end, end_idx = make_sequences(X_all_scaled, y_all, t_all, lookback=lookback)  # sequences for LSTM [25][27]

# Translate split boundaries into sequence space based on window end indices
train_mask = end_idx < train_end  # windows ending in train segment [21]
val_mask   = (end_idx >= train_end) & (end_idx < val_end)  # windows ending in val segment [21]
test_mask  = end_idx >= val_end  # windows ending in test segment [21]

X_train_seq, y_train_seq, t_train_seq_end = X_seq_all[train_mask], y_seq_all[train_mask], t_seq_end[train_mask]  # train sequences [25]
X_val_seq,   y_val_seq,   t_val_seq_end   = X_seq_all[val_mask],   y_seq_all[val_mask],   t_seq_end[val_mask]    # val sequences [25]
X_test_seq,  y_test_seq,  t_test_seq_end  = X_seq_all[test_mask],  y_seq_all[test_mask],  t_seq_end[test_mask]   # test sequences [25]

# 6) Sanity checks
print("Flat (pre-sequence) shapes:")
print("  X_train:", X_train.shape, "X_val:", X_val.shape, "X_test:", X_test.shape)  # verify 70/15/15 [21]
print("  y_train:", y_train.shape, "y_val:", y_val.shape, "y_test:", y_test.shape)  # targets aligned [21]
print("Time ranges (flat):")
print("  Train:", t_train.min(), "->", t_train.max(), "| Val:", t_val.min(), "->", t_val.max(), "| Test:", t_test.min(), "->", t_test.max())  # ranges [24]

print("\nSequence (LSTM) shapes:")
print("  X_train_seq:", X_train_seq.shape, "y_train_seq:", y_train_seq.shape)       # (samples, timesteps, features) [25]
print("  X_val_seq:",   X_val_seq.shape,   "y_val_seq:",   y_val_seq.shape)         # validation sequences [25]
print("  X_test_seq:",  X_test_seq.shape,  "y_test_seq:",  y_test_seq.shape)        # test sequences [25]
print("Sequence end time ranges:")
print("  Train end:", t_train_seq_end.min(), "->", t_train_seq_end.max())
print("  Val   end:", t_val_seq_end.min(),   "->", t_val_seq_end.max())
print("  Test  end:", t_test_seq_end.min(),  "->", t_test_seq_end.max())

"""### 2.1.1 check class imbalance"""

np.bincount(y_train_seq[:, 0].astype(int))

"""### 2.1.2 compute class weight for fire occur"""

from sklearn.utils import class_weight
import numpy as np

# Extract the fire occurrence column (0 or 1)
occ_y_train = y_train_seq[:, 0].astype(int)

# Compute balanced class weights
weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(occ_y_train),
    y=occ_y_train
)

# Convert to dictionary format expected by Keras
class_weights_dict = dict(enumerate(weights))

print("Class weights for fire_occurrence:", class_weights_dict)

"""### 2.1.3 Class weight trianed"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

lookback = X_train_seq.shape[1]       # 48
n_features = X_train_seq.shape[2]     # 4

inputs = keras.Input(shape=(lookback, n_features), name="seq_in")

x = layers.SpatialDropout1D(0.2)(inputs)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)
x = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(x)

attn_out = layers.MultiHeadAttention(num_heads=2, key_dim=32)(x, x)
x = layers.Add()([x, attn_out])
x = layers.LayerNormalization()(x)

x = layers.GlobalAveragePooling1D()(x)
x = layers.Dropout(0.2)(x)
x = layers.Dense(64, activation="relu")(x)

# Output layer: binary classification (fire_occurrence)
output = layers.Dense(1, activation="sigmoid", name="fire_occurrence")(x)

model = keras.Model(inputs=inputs, outputs=output)
model.compile(
    optimizer="adam",
    loss="binary_crossentropy",
    metrics=["accuracy", tf.keras.metrics.AUC(name='auc')]
)

model.summary()

"""### 2.1.4 Run the model"""

history = model.fit(
    X_train_seq,
    y_train_seq[:, 0],
    validation_data=(X_val_seq, y_val_seq[:, 0]),
    epochs=50,
    batch_size=32,
    class_weight={0: 0.5047, 1: 53.69},
    callbacks=[
        keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True),
        keras.callbacks.ReduceLROnPlateau(patience=3, factor=0.5)
    ]
)

test_results = model.evaluate(X_test_seq, y_test_seq[:, 0])
print("Test results:", dict(zip(model.metrics_names, test_results)))

"""### 2.1.5 Printing the accuracy matrix"""

from sklearn.metrics import confusion_matrix, classification_report

# Predict probabilities
y_probs = model.predict(X_test_seq)

# Threshold to get class predictions (default 0.5)
y_preds = (y_probs.flatten() > 0.5).astype(int)

# Ground truth
y_true = y_test_seq[:, 0].astype(int)

# Confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_true, y_preds))

# Classification report (includes precision, recall, F1)
print("\nClassification Report:")
print(classification_report(y_true, y_preds, digits=4))

"""## 2.2 Model architecture: Bi-directional Stat LSTM"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Shapes
lookback = X_train_seq.shape[1]     # timesteps
n_features = X_train_seq.shape[2]   # input features

# ===== Input layer =====
inputs = keras.Input(shape=(lookback, n_features), name="seq_in")  # Input layer: (timesteps, features)

# ===== Hidden layers =====
# Regularize feature maps across timesteps
x = layers.SpatialDropout1D(0.2, name="spatialdrop_1")(inputs)  # Hidden layer: SpatialDropout1D

# Stacked bidirectional LSTMs (first returns sequences for attention)
x = layers.Bidirectional(layers.LSTM(64, return_sequences=True), name="bilstm_1")(x)  # Hidden layer: BiLSTM
x = layers.Bidirectional(layers.LSTM(32, return_sequences=True), name="bilstm_2")(x)  # Hidden layer: BiLSTM (stack)

# Self-attention over the sequence
attn_out = layers.MultiHeadAttention(num_heads=2, key_dim=32, name="mha_self")(x, x)  # Hidden layer: MultiHeadAttention

# Residual fusion + normalization
x = layers.Add(name="attn_residual")([x, attn_out])  # Hidden layer: residual add
x = layers.LayerNormalization(name="attn_ln")(x)     # Hidden layer: normalization

# Pool sequence to a fixed-size vector
x = layers.GlobalAveragePooling1D(name="gap")(x)     # Hidden layer: pooling
x = layers.Dropout(0.2, name="drop_post")(x)         # Hidden layer: dropout
x = layers.Dense(64, activation="relu", name="dense_1")(x)  # Hidden layer: dense

# ===== Output layers =====
occ_out = layers.Dense(1, activation="sigmoid", name="occ_out")(x)     # Output layer: fire_occurrence (binary)
cnt_out = layers.Dense(1, activation="softplus", name="cnt_out")(x)    # Output layer: fire_count (non-negative)

model = keras.Model(inputs=inputs, outputs=[occ_out, cnt_out], name="stacked_bilstm_attention")
model.summary()

"""## 2.3 Compile model"""

# Split multi-output targets
y_train_occ = y_train_seq[:, 0].astype('float32')
y_train_cnt = y_train_seq[:, 1].astype('float32')

y_val_occ = y_val_seq[:, 0].astype('float32')
y_val_cnt = y_val_seq[:, 1].astype('float32')

y_test_occ = y_test_seq[:, 0].astype('float32')
y_test_cnt = y_test_seq[:, 1].astype('float32')

# Compile with multi-loss
model.compile(
    optimizer=keras.optimizers.Adam(1e-3),
    loss={"occ_out": keras.losses.BinaryCrossentropy(from_logits=False),
          "cnt_out": keras.losses.Poisson()},
    loss_weights={"occ_out": 1.0, "cnt_out": 1.0},
    metrics={"occ_out": [keras.metrics.AUC(name="auc"),
                         keras.metrics.BinaryAccuracy(name="acc")],
             "cnt_out": [keras.metrics.MeanAbsoluteError(name="mae")]}
)

# Early stopping and LR scheduling
early_stop = keras.callbacks.EarlyStopping(
    monitor="val_loss", mode="min", patience=6, restore_best_weights=True, verbose=1
)
reduce_lr = keras.callbacks.ReduceLROnPlateau(
    monitor="val_loss", mode="min", factor=0.5, patience=3, min_lr=1e-6, verbose=1
)

# Train up to 50 epochs
history = model.fit(
    X_train_seq,
    {"occ_out": y_train_occ, "cnt_out": y_train_cnt},
    validation_data=(X_val_seq, {"occ_out": y_val_occ, "cnt_out": y_val_cnt}),
    epochs=50,
    batch_size=32,
    callbacks=[early_stop, reduce_lr],
    verbose=1
)

# Evaluate on test
eval_res = model.evaluate(
    X_test_seq, {"occ_out": y_test_occ, "cnt_out": y_test_cnt}, verbose=1, return_dict=True
)
print("Test results:", eval_res)

# Predict (optional)
y_pred_occ, y_pred_cnt = model.predict(X_test_seq, verbose=0)

from sklearn.utils import class_weight

# Compute weights automatically
class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(y_train_seq[:, 0]),
    y=y_train_seq[:, 0]
)

# Apply only to occ_out
loss_weights = {"occ_out": 1.0, "cnt_out": 1.0}  # You can adjust

model.fit(
    X_train_seq,
    {"occ_out": y_train_seq[:, 0], "cnt_out": y_train_seq[:, 1]},
    validation_data=(
        X_val_seq,
        {"occ_out": y_val_seq[:, 0], "cnt_out": y_val_seq[:, 1]}
    ),
    class_weight={"occ_out": dict(enumerate(class_weights))},  # Apply to classification head
    epochs=50,
    batch_size=32,
    callbacks=[...],
)

"""##"""