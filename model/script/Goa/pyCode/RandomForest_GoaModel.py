# -*- coding: utf-8 -*-
"""RandomForest_GoaModel.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mS_i54IwUI6TX97BEL2p48mMUd-24UiB

# (1) Import req module and data

## 1.1 Import the final merged cleandata of goa
"""

# Importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load Firepoints Data

Fianl_cleanData_Merged_Goa = pd.read_csv("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/Final_Merge_CleanData_1Hour.csv", encoding='latin1')

"""## 1.2 Load the shape file of Goa"""

import geopandas as gpd

# Load the shape file
protect_area = gpd.read_file("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/Data/shp/Protect_area.shp")

# Prview spatial data
print(protect_area.head())
protect_area.plot()

"""## 1.3 Drop the rendundant column"""

# Drop Forest Block, Compartment number columns

Fianl_cleanData_Merged_Goa.drop(columns=['DT_1H','DISTRICT_N','FIRE_DT_1H_x','FIRE_DT_1H_y'], inplace=True)
print(Fianl_cleanData_Merged_Goa.head())

"""# (2) Implement ML Model

## 2.1 Splitting data
"""

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the final table (use in-memory if present, else CSV)
df = Fianl_cleanData_Merged_Goa.copy()

# Safety check
assert "fire_occurrence" in df.columns, "Label column 'fire_occurrence' not found"

# y: target
y = df["fire_occurrence"].astype(int)

# X: all numeric features except the label
# This automatically ignores textual columns like station, district, circle, etc.
X_num = df.select_dtypes(include=["number"]).copy()
if "fire_occurrence" in X_num.columns:
    X_num = X_num.drop(columns=["fire_occurrence"])

# Report class balance before split
print("Total rows:", len(df))
print("Label distribution:\n", y.value_counts(dropna=False))

# First split: 60% train, 40% temp (stratified)
X_train, X_temp, y_train, y_temp = train_test_split(
    X_num, y, test_size=0.40, stratify=y, random_state=42, shuffle=True
)

# Second split: temp -> 20% val, 20% test (50/50 split of temp, stratified)
X_val, X_test, y_val, y_test = train_test_split(
    X_temp, y_temp, test_size=0.50, stratify=y_temp, random_state=42, shuffle=True
)

# Show shapes and class balance
def info_split(name, Xp, yp):
    print(f"{name}: X={Xp.shape}, y={yp.shape}")
    print(f"{name} label distribution:\n", yp.value_counts(dropna=False))

info_split("Train", X_train, y_train)
info_split("Validation", X_val, y_val)
info_split("Test", X_test, y_test)

"""## 2.2 Model Selection"""

import copy
import numpy as np
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import average_precision_score

# Random Forest with class balancing and warm_start for incremental training
rf = RandomForestClassifier(
    n_estimators=0,                # will grow trees iteratively
    criterion="log_loss",         # probabilistic criterion for classification
    class_weight="balanced_subsample",
    max_depth=None,
    min_samples_split=2,
    min_samples_leaf=1,
    max_features="sqrt",
    n_jobs=-1,
    random_state=42,
    warm_start=True               # allow adding trees across epochs
)

model = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),  # RF cannot handle NaNs directly
    ("rf", rf)
])

# Early stopping config
max_epochs = 30                  # requested number of runs
trees_per_epoch = 10             # trees added per epoch (total up to 300)
patience = 5                     # stop if no improvement for 'patience' epochs
min_delta = 1e-4                 # minimum AP improvement to reset patience

best_ap = -np.inf
best_epoch = -1
epochs_without_improve = 0
best_model = None

for epoch in range(1, max_epochs + 1):
    # Increase the number of trees
    model.named_steps["rf"].n_estimators += trees_per_epoch

    # Fit on training data (imputer fits on train only inside the pipeline)
    model.fit(X_train, y_train)

    # Evaluate on validation with Average Precision (PR AUC) for imbalanced data
    val_proba = model.predict_proba(X_val)[:, 1]
    ap = average_precision_score(y_val, val_proba)

    print(f"Epoch {epoch:02d} | n_estimators={model.named_steps['rf'].n_estimators} | Val AP={ap:.6f}")

    # Early stopping check
    if ap > best_ap + min_delta:
        best_ap = ap
        best_epoch = epoch
        best_model = copy.deepcopy(model)  # snapshot the current best model
        epochs_without_improve = 0
    else:
        epochs_without_improve += 1
        if epochs_without_improve >= patience:
            print(f"Early stopping at epoch {epoch} (best at epoch {best_epoch} with AP={best_ap:.6f})")
            break

# Use the best snapshot for downstream steps
rf_model = best_model if best_model is not None else model
print(f"Training complete. Best epoch={best_epoch}, Best Val AP={best_ap:.6f}")

"""## 2.3 Print the accuracy of model"""

import numpy as np
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    balanced_accuracy_score, roc_auc_score, average_precision_score,
    confusion_matrix, classification_report, brier_score_loss, log_loss,
    mean_squared_error, r2_score
)

# Probabilities and class predictions
val_proba = rf_model.predict_proba(X_val)[:, 1]
test_proba = rf_model.predict_proba(X_test)[:, 1]

val_pred = (val_proba >= 0.5).astype(int)
test_pred = (test_proba >= 0.5).astype(int)

def print_metrics(split_name, y_true, y_pred, y_proba):
    # Core classification metrics for imbalanced data
    acc  = accuracy_score(y_true, y_pred)
    bal  = balanced_accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec  = recall_score(y_true, y_pred, zero_division=0)
    f1   = f1_score(y_true, y_pred, zero_division=0)
    roc  = roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) == 2 else np.nan
    ap   = average_precision_score(y_true, y_proba)

    # Calibration- and error-like quantities on probabilities
    brier = brier_score_loss(y_true, y_proba)
    ll    = log_loss(y_true, np.vstack([1 - y_proba, y_proba]).T, labels=[0, 1])

    # Regression-style errors on probability targets (non-standard but informative)
    mse  = mean_squared_error(y_true, y_proba)
    rmse = np.sqrt(mse)
    r2   = r2_score(y_true, y_proba)

    # Confusion matrix
    cm = confusion_matrix(y_true, y_pred, labels=[0, 1])

    print(f"\n=== {split_name} Metrics ===")
    print(f"Accuracy:               {acc:.6f}")
    print(f"Balanced Accuracy:      {bal:.6f}")
    print(f"Precision (pos=1):      {prec:.6f}")
    print(f"Recall (pos=1):         {rec:.6f}")
    print(f"F1 (pos=1):             {f1:.6f}")
    print(f"ROC AUC:                {roc:.6f}")
    print(f"Average Precision (AP): {ap:.6f}")
    print(f"Brier score (prob):     {brier:.6f}")
    print(f"Log loss:               {ll:.6f}")
    print(f"MSE (prob vs label):    {mse:.6f}")
    print(f"RMSE (prob vs label):   {rmse:.6f}")
    print(f"R2 (prob vs label):     {r2:.6f}")
    print("Confusion Matrix [rows=true 0/1, cols=pred 0/1]:")
    print(cm)
    print("\nClassification report:")
    print(classification_report(y_true, y_pred, digits=6, zero_division=0))

# Print for Validation and Test
print_metrics("Validation", y_val, val_pred, val_proba)
print_metrics("Test",       y_test, test_pred, test_proba)