# -*- coding: utf-8 -*-
"""FF_DataCleaning script for Goa region.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15geMvOF-oAFgGh1uFGlTyd8JnLJe7v1C

# 1. Load the dataset
"""

# Importing libraries

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load Firepoints Data

goa_fire = pd.read_csv("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/Data/FSI-Firepoints-100625 Goa.csv", encoding='latin1')

# Load weather data

goa_weather = pd.read_csv("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/Data/GOA_AWS_IMD_data.csv")

# Load AWS location

aws_locations = pd.read_csv("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/Data/All AWS Location.csv")

# Preview the firepoints data
print("---------------Firepoints data:-----------------")
print(goa_fire.head())


# Previw the weather data
print("----------------Weather data:---------------------")
print(goa_weather.head())

# Preview the aws locations
print("------------------AWS locations:------------------")
print(aws_locations.head())

"""## 1.1 Load the shape file of Goa"""

import geopandas as gpd

# Load the shape file
protect_area = gpd.read_file("/content/drive/MyDrive/BITS_PILANI_RESEARCH/Data/Data/shp/Protect_area.shp")

# Prview spatial data
print(protect_area.head())
protect_area.plot()

"""# 2. Data Cleaning and Preprocessing

## 2.1 Handling missing values in firespoints data
"""

# Checking missing values in firepoints

print("Missing values in Goa Firepoints:\n", goa_fire.isnull().sum())

# Drop the unnamed:14 which is a junk column in every firepoints

goa_fire.drop(columns=['Unnamed: 14'], inplace=True)

# Drop Forest Block, Compartment number columns

goa_fire.drop(columns=['Source','State','Division','Range','Forest Block', 'Compartment No'], inplace=True)

"""## 2.2 Handling misssing values in Weather and AWS data"""

# Checking Missing values in goa_weather data

print("Missing values in Goa Weather data:\n", goa_weather.isnull().sum(),"\n")
print("Info of goa aws data:", goa_weather.info())

# Checking missing values in aws data
print("Missing values in AWS Locations:\n", aws_locations.isnull().sum(),"\n")

# Drop Serial number from aws location
aws_locations.drop(columns=['S NO.'], inplace=True)

print(aws_locations.info())

"""### 2.2.1 Trim Goa weather data to match the firepoints date range"""

# Convert to datetime if not already
goa_weather['DATE(YYYY-MM-DD)'] = pd.to_datetime(goa_weather['DATE(YYYY-MM-DD)'], errors='coerce')

# Define date range with buffer
start_date = pd.to_datetime('2021-10-01')
end_date = pd.to_datetime('2024-06-01')

# Filter the weather data within range
goa_weather_trimmed = goa_weather[(goa_weather['DATE(YYYY-MM-DD)'] >= start_date) &
                                  (goa_weather['DATE(YYYY-MM-DD)'] <= end_date)]

# Optional: Reset index
goa_weather_trimmed = goa_weather_trimmed.reset_index(drop=True)

# Check result
print(f"Trimmed weather data from {start_date.date()} to {end_date.date()}")
print(goa_weather_trimmed['DATE(YYYY-MM-DD)'].min(), goa_weather_trimmed['DATE(YYYY-MM-DD)'].max())
print(goa_weather_trimmed.head())

# Checking Missing values in goa_weather_trimmed data

print("Missing values in Goa Weather data:\n", goa_weather_trimmed.isnull().sum(),"\n")

# Drop columns with High Missigness

drop_cols = [
    "S NO.",
    "RH DAY MIN MAX (%)",
    "WIND DIR 3 m (Deg)", "WIND SPEED 3 m (Kt)", "WIND SPEED MAX / GUST 3 m (Kt)",
    "SUN SHINE (HH.MM)", "SOIL TEMP 10 cm ('C)",
    "SOIL TEMP 30 cm ('C)", "SOIL MOIS 30 cm (m3 / m3)",
    "SOIL TEMP 70 cm ('C)", "SOIL MOIS 70 cm (m3 / m3)",
    "SOIL TEMP 100 cm ('C)", "SOIL MOIS 100 cm (m3 / m3)",
    "GLOBAL RAD IATION (v / m2)", "PAR (micro - mole / m2s)",
    "BATTERY (Volts)","GPS"
]
goa_weather_trimmed = goa_weather_trimmed.drop(columns=drop_cols, errors='ignore')
print("Missing values in Goa Weather data:\n", goa_weather_trimmed.isnull().sum(),"\n")
print("Info of goa aws data:", goa_weather_trimmed.info())
print(goa_weather_trimmed.head())

"""#### 2.2.1.2 Drop the rows with missing Temp"""

goa_weather_trimmed.dropna(subset=["TEMP. ('C)"], inplace=True)
print("Missing values in Goa Weather data:\n", goa_weather_trimmed.isnull().sum(),"\n")

# Group by station and date, then get min and max temps per station per day
daily_min_temps = goa_weather_trimmed.groupby(['STATION', 'DATE(YYYY-MM-DD)'])["TEMP DAY MIN. ('C)"].transform('min')
daily_max_temps = goa_weather_trimmed.groupby(['STATION', 'DATE(YYYY-MM-DD)'])["TEMP DAY MAX. ('C)"].transform('max')

# Assign these per station-date min/max values back to all rows of same station-date
goa_weather_trimmed["TEMP DAY MIN. ('C)"] = daily_min_temps
goa_weather_trimmed["TEMP DAY MAX. ('C)"] = daily_max_temps

"""### 2.2.2 Fill the missing values in the weather data using interpolation"""

interp_cols = [
    "RAIN FALL CUM. SINCE 0300 UTC (mm)",
    "RH (%)",
    "WIND DIR 10 m (Deg)",
    "WIND SPEED 10 m (Kt)",
    "WIND SPEED MAX / GUST 10 m (Kt)",
    "SLP (hPa)",
    "MSLP (hPa / gpm)",
    "SOIL MOIS 10 cm (m3 / m3)"
]

# Ensure the data is sorted within each station by datetime before interpolating
goa_weather_trimmed = goa_weather_trimmed.sort_values(['DATE(YYYY-MM-DD)', 'TIME (UTC)', 'STATION'])

for col in interp_cols:
    goa_weather_trimmed[col] = (
        goa_weather_trimmed.groupby('STATION')[col]
        .apply(lambda group: group.interpolate(method='linear'))
        .reset_index(level=0, drop=True)
    )

print("Missing values in Goa Weather data:\n", goa_weather_trimmed.isnull().sum(),"\n")

# Sort back to correct order: station, date, then time
goa_weather_trimmed = goa_weather_trimmed.sort_values([ 'DATE(YYYY-MM-DD)','STATION', 'TIME (UTC)'])

# Reset the index to clean integer index
goa_weather_trimmed = goa_weather_trimmed.reset_index(drop=True)

# Verify the sort and index reset
print(goa_weather_trimmed.head())

"""### 2.2.3 Fill the missing values using mean method"""

columns_to_fill = [
    "SLP (hPa)",
    "MSLP (hPa / gpm)",
    "SOIL MOIS 10 cm (m3 / m3)"
]

for col in columns_to_fill:
    if col in goa_weather_trimmed.columns:
        goa_weather_trimmed[col].fillna(goa_weather_trimmed[col].mean(), inplace=True)

print("Missing values in Goa Weather data:\n", goa_weather_trimmed.isnull().sum(),"\n")

"""### 2.2.4 Fill the rest missing values of T_Min and T_Max according Temp"""

# Define a function to fill missing min/max temp using temp column stats per group
def fill_min_max(group):
    if group["TEMP DAY MIN. ('C)"].isnull().any():
        group["TEMP DAY MIN. ('C)"] = group["TEMP DAY MIN. ('C)"].fillna(group["TEMP. ('C)"].min())
    if group["TEMP DAY MAX. ('C)"].isnull().any():
        group["TEMP DAY MAX. ('C)"] = group["TEMP DAY MAX. ('C)"].fillna(group["TEMP. ('C)"].max())
    return group

# Apply group-wise filling
goa_weather_trimmed = goa_weather_trimmed.groupby(['STATION', 'DATE(YYYY-MM-DD)']).apply(fill_min_max)

print("Missing values in Goa Weather data:\n", goa_weather_trimmed.isnull().sum(),"\n")

"""## 2.3 Convert coordinates into DMS"""

# Converting latitudes and longitudes of goa_fire into decimal

import re

def dms_to_decimal(dms_str):
    # Regex pattern to capture degrees, minutes, seconds, direction with optional spaces and both types of quotes
    pattern = r"(\d+)°\s*(\d+)'?\s*(\d+)(?:\"|”)?\s*([NSEW])"
    match = re.match(pattern, dms_str.strip())
    if not match:
        return None
    degrees, minutes, seconds, direction = match.groups()
    dec = float(degrees) + float(minutes)/60 + float(seconds)/3600
    if direction in ['S', 'W']:
        dec = -dec
    return dec

# Apply conversion functions
goa_fire['Latitude'] = goa_fire['Latitude'].apply(dms_to_decimal)
goa_fire['Longitude'] = goa_fire['Longitude'].apply(dms_to_decimal)

print(goa_fire[['Latitude', 'Longitude']])

print(goa_fire.info())
print(goa_fire.head())

"""## 2.4 Renaming the lat long columns of FSI and AWS:"""

# Rename columns in FSI dataset
goa_fire = goa_fire.rename(columns={
    'Latitude': 'Lat_FSI',
    'Longitude': 'Long_FSI'
})

# Rename columns in AWS dataset
aws_locations = aws_locations.rename(columns={
    'LATITUDE': 'Lat_AWS',
    'LONGITUDE': 'Long_AWS'
})

print(goa_fire.columns)
print(aws_locations.columns)

"""### 2.4.1 Fill NaN values of AWS_location altitude"""

# Impute missing altitude values with median (robust to outliers)
aws_locations['ALTITUDE'].fillna(aws_locations['ALTITUDE'].median(), inplace=True)

print("Missing values in Goa AWS_location:\n", aws_locations.isnull().sum(),"\n")

print(aws_locations.head())

"""# 3 Merge the datasets to make a single dataset

## 3.1 Check for the unique station in weather data and aws location
"""

# Print unique station name of goa_weather_trimmed
unique_station_goa_weather = goa_weather_trimmed['STATION'].unique()
print("Unique station of goa weather:- \n",unique_station_goa_weather)

# Print station name of aws_location
all_aws_location = aws_locations['STATION']
print("\nAll AWS station name:- \n",all_aws_location)

"""## 3.1 On basis of station column merge aws_location and goa_weather"""

# Standardize station names for matching
goa_weather_trimmed['STATION_CLEAN'] = goa_weather_trimmed['STATION'].str.strip().str.upper()
aws_locations['STATION_CLEAN'] = aws_locations['STATION'].str.strip().str.upper()

# List of matching stations
matching_stations = set(aws_locations['STATION_CLEAN']) & set(goa_weather_trimmed['STATION_CLEAN'])
print(f"Matching stations: {matching_stations}")

# Filter goa_weather_trimmed to only stations that exist in aws_locations
goa_weather_matched = goa_weather_trimmed[goa_weather_trimmed['STATION_CLEAN'].isin(matching_stations)]

# Merge with aws_locations using the cleaned station names
merged_goa_weather_aws = goa_weather_matched.merge(
    aws_locations,
    on='STATION_CLEAN',
    how='left',
    suffixes=('', '_AWS')
).drop(columns=['STATION_CLEAN'])

print("After merge, sample data:")
print(merged_goa_weather_aws.head())

"""### 3.1.1 Drop uneccessary columns of the merged_goa_weather"""

# Drop one of the station,district column to make a single column of STATION and DISTRICT
merged_goa_weather_aws = merged_goa_weather_aws.drop(columns=['STATION_AWS','DISTRICT_AWS'])

# Drop columns like STATE(which is known Goa), Type of AWS
merged_goa_weather_aws = merged_goa_weather_aws.drop(columns=['STATE','TYPE'])

# Print final merge_goa_weather_aws dataset
print("Final merged_goa_weather_aws, sample data:")
print(merged_goa_weather_aws.head())

"""### 3.1.2 Download the Merged_GoaWeather_With_AwsLocation.csv"""

# merged_goa_weather_aws.to_csv('Merged_GoaWeather_With_AwsLocation.csv', index=False)

print(goa_fire.info())
print(merged_goa_weather_aws.info())

print(goa_fire.head(19))

"""## 3.2 On the basis of district, date and time merge the goa_fire and merged_goaweather_with_awslocation with oneHotEncoded fire occur or not occur"""

import pandas as pd
import re

# Inputs assumed in memory: merged_goa_weather_aws, goa_fire

# 1) Normalize and hour-bucket keys
norm = lambda s: re.sub(r"\s+", " ", str(s)).strip().upper().replace(" ", "_")

merged_goa_weather_aws["DT"] = pd.to_datetime(
    merged_goa_weather_aws["DATE(YYYY-MM-DD)"].dt.strftime("%Y-%m-%d")
    + " " + merged_goa_weather_aws["TIME (UTC)"],
    errors="coerce", utc=False
)
merged_goa_weather_aws["DT_1H"] = merged_goa_weather_aws["DT"].dt.floor("h")
merged_goa_weather_aws["DISTRICT_N"] = merged_goa_weather_aws["DISTRICT"].apply(norm)

goa_fire["FIRE_DT"] = pd.to_datetime(
    goa_fire["Fire Date"] + " " + goa_fire["Fire Time"],
    errors="coerce", utc=False
)
goa_fire["FIRE_DT_1H"] = goa_fire["FIRE_DT"].dt.floor("h")
goa_fire["DISTRICT_N"] = goa_fire["District"].apply(norm)

# 2) Label-first: left-join unique fire hour keys to weather
fire_keys = goa_fire[["DISTRICT_N", "FIRE_DT_1H"]].drop_duplicates().assign(fire_occurrence=1)
labelled = merged_goa_weather_aws.merge(
    fire_keys,
    how="left",
    left_on=["DISTRICT_N", "DT_1H"],
    right_on=["DISTRICT_N", "FIRE_DT_1H"]
)
labelled["fire_occurrence"] = labelled["fire_occurrence"].fillna(0).astype(int)

# 3) Attach scalar fire attributes (pick deterministic reducer; here: first)
attr_first = (
    goa_fire.groupby(["DISTRICT_N", "FIRE_DT_1H"], dropna=False)
            .agg({
                "Lat_FSI": "first",
                "Long_FSI": "first",
                "Circle": "first",
                "Block/Section/Round": "first",
                "Beat": "first",
            })
            .reset_index()
)

Final_OneRowPerWeather = labelled.merge(
    attr_first,
    how="left",
    left_on=["DISTRICT_N", "DT_1H"],
    right_on=["DISTRICT_N", "FIRE_DT_1H"]
)

# 4) Optional: drop helper right key
Final_OneRowPerWeather = Final_OneRowPerWeather.drop(columns=["FIRE_DT_1H"], errors="ignore")

# 5) Sanity checks
print("Label counts:")
print(Final_OneRowPerWeather["fire_occurrence"].value_counts(dropna=False))
print(Final_OneRowPerWeather.head())

"""## 3.3 Downlod the final merge clean csv"""

# Final_OneRowPerWeather.to_csv('Fianl_cleanData_Merged_Goa.csv', index=False)

# Count occurrences of 0 and 1 in the fire occurrence column
counts = Final_OneRowPerWeather['fire_occurrence'].value_counts()

print("Number of 1s (fire occurred):", counts.get(1, 0))
print("Number of 0s (no fire):", counts.get(0, 0))

"""## 3.4 Unreq 15 min data interval setup"""

# import pandas as pd

# # Build timestamps
# merged_goa_weather_aws["DT"] = pd.to_datetime(
#     merged_goa_weather_aws["DATE(YYYY-MM-DD)"].dt.strftime("%Y-%m-%d") + " " + merged_goa_weather_aws["TIME (UTC)"]
# )
# merged_goa_weather_aws["DT_15"] = merged_goa_weather_aws["DT"].dt.floor("15min")

# goa_fire["FIRE_DT"] = pd.to_datetime(goa_fire["Fire Date"] + " " + goa_fire["Fire Time"])
# goa_fire["FIRE_DT_15"] = goa_fire["FIRE_DT"].dt.floor("15min")

# # Normalize district
# norm = lambda s: str(s).strip().upper().replace(" ", "_")
# merged_goa_weather_aws["DISTRICT_N"] = merged_goa_weather_aws["DISTRICT"].map(norm)
# goa_fire["DISTRICT_N"] = goa_fire["District"].map(norm)

# # Aggregate goa_fire attributes per key into lists
# agg_cols = {
#     "Lat_FSI": list,
#     "Long_FSI": list,
#     "Circle": lambda x: list(pd.unique(x)),
#     "Block/Section/Round": lambda x: list(pd.unique(x)),
#     "Beat": lambda x: list(pd.unique(x)),
# }
# fire_agg = (
#     goa_fire.groupby(["DISTRICT_N", "FIRE_DT_15"], dropna=False)
#         .agg(agg_cols)
#         .reset_index()
#         .assign(fire_occurrence=1)
# )

# # Merge aggregated goa_fire info into merged_goa_weather_aws
# Final_Merge_CleanData = merged_goa_weather_aws.merge(
#     fire_agg,
#     how="left",
#     left_on=["DISTRICT_N", "DT_15"],
#     right_on=["DISTRICT_N", "FIRE_DT_15"]
# )

# # Binary label
# Final_Merge_CleanData["fire_occurrence"] = Final_Merge_CleanData["fire_occurrence"].fillna(0).astype(int)

# # Drop helpers and save
# drop_cols = ["DISTRICT_N", "DT", "DT_15", "FIRE_DT_15"]
# final_cols = [c for c in Final_Merge_CleanData.columns if c not in drop_cols]
# final = Final_Merge_CleanData[final_cols].copy()

# print(Final_Merge_CleanData.head())
# print(Final_Merge_CleanData.info())

# import pandas as pd

# # 1) Build 15-min timestamps
# merged_goa_weather_aws["DT"] = pd.to_datetime(
#     merged_goa_weather_aws["DATE(YYYY-MM-DD)"].dt.strftime("%Y-%m-%d") + " " + merged_goa_weather_aws["TIME (UTC)"]
# )
# merged_goa_weather_aws["DT_15"] = merged_goa_weather_aws["DT"].dt.floor("15min")

# goa_fire["FIRE_DT"] = pd.to_datetime(goa_fire["Fire Date"] + " " + goa_fire["Fire Time"])
# goa_fire["FIRE_DT_15"] = goa_fire["FIRE_DT"].dt.floor("15min")

# # 2) Normalize district strings to a common style (UPPER + underscores)
# norm = lambda s: str(s).strip().upper().replace(" ", "_")
# merged_goa_weather_aws["DISTRICT_N"] = merged_goa_weather_aws["DISTRICT"].map(norm)
# goa_fire["DISTRICT_N"] = goa_fire["District"].map(norm)

# # 3) Unique goa_fire keys at district × 15-min
# fire_keys = (
#     goa_fire[["DISTRICT_N", "FIRE_DT_15"]]
#     .drop_duplicates()
#     .assign(fire_occurrence=1)
# )

# # 4) Left-merge goa_fire flags into merged_goa_weather_aws (no merged_goa_weather_aws rows dropped)
# Final_Merge_CleanData = merged_goa_weather_aws.merge(
#     fire_keys,
#     how="left",
#     left_on=["DISTRICT_N", "DT_15"],
#     right_on=["DISTRICT_N", "FIRE_DT_15"]
# )
# Final_Merge_CleanData["fire_occurrence"] = Final_Merge_CleanData["fire_occurrence"].fillna(0).astype(int)

# # 5) Keep original merged_goa_weather_aws columns + label
# drop_cols = ["DISTRICT_N", "DT", "DT_15", "FIRE_DT_15"]
# final_cols = [c for c in Final_Merge_CleanData.columns if c not in drop_cols]
# final = Final_Merge_CleanData[final_cols].copy()

# print(Final_Merge_CleanData.head())
# print(Final_Merge_CleanData.info())

# import pandas as pd
# import numpy as np

# # Step 1: Convert Fire Date and Fire Time in goa_fire to datetime
# goa_fire['Fire Date'] = pd.to_datetime(goa_fire['Fire Date'], errors='coerce')

# # Combine Fire Date and Fire Time into a single datetime column for easier rounding
# goa_fire['Fire DateTime'] = pd.to_datetime(
#     goa_fire['Fire Date'].dt.strftime('%Y-%m-%d') + ' ' + goa_fire['Fire Time'],
#     errors='coerce'
# )

# # Round Fire DateTime to nearest 15 minutes
# goa_fire['Fire DateTime'] = goa_fire['Fire DateTime'].dt.round('15min')

# # Separate date and time for merging
# goa_fire['DATE(YYYY-MM-DD)'] = goa_fire['Fire DateTime'].dt.date
# goa_fire['TIME (UTC)'] = goa_fire['Fire DateTime'].dt.time

# # Step 2: Ensure DATE(YYYY-MM-DD) in merged_goa_weather_aws is of date type
# if not pd.api.types.is_datetime64_any_dtype(merged_goa_weather_aws['DATE(YYYY-MM-DD)']):
#     merged_goa_weather_aws['DATE(YYYY-MM-DD)'] = pd.to_datetime(merged_goa_weather_aws['DATE(YYYY-MM-DD)'], errors='coerce')

# # Convert to date (remove time)
# merged_goa_weather_aws['DATE(YYYY-MM-DD)'] = merged_goa_weather_aws['DATE(YYYY-MM-DD)'].dt.date

# # Convert TIME (UTC) in merged_goa_weather_aws to datetime.time
# merged_goa_weather_aws['TIME (UTC)'] = pd.to_datetime(merged_goa_weather_aws['TIME (UTC)'], errors='coerce')
# # Round to nearest 15 minutes
# merged_goa_weather_aws['TIME (UTC)'] = merged_goa_weather_aws['TIME (UTC)'].dt.round('15min').dt.time

# # Standardize District names (uppercase, no leading/trailing spaces)
# goa_fire['District'] = goa_fire['District'].str.strip().str.upper()
# merged_goa_weather_aws['DISTRICT'] = merged_goa_weather_aws['DISTRICT'].str.strip().str.upper()

# # Merge with indicator to see which rows matched
# Final_Merge_CleanData = pd.merge(
#     merged_goa_weather_aws,
#     goa_fire[['District', 'DATE(YYYY-MM-DD)', 'Fire Time']],
#     on=['DISTRICT', 'DATE(YYYY-MM-DD)', 'TIME (UTC)'],
#     how='left',
#     indicator=True
# )

# # Create fire_occurred column: 1 if fire record matched, else 0
# Final_Merge_CleanData['fire_occurred'] = np.where(Final_Merge_CleanData['_merge'] == 'both', 1, 0)

# # Drop the merge indicator column
# Final_Merge_CleanData.drop(columns=['_merge'], inplace=True)

# # Verify
# print(Final_Merge_CleanData.head())